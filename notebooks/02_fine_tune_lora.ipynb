{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mgfrantz/CalTech-CTME-AramCo-2025/blob/main/notebooks/02_fine_tune_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA9sUDBfwiM9"
   },
   "source": [
    "# Part 1: Dataset Preparation\n",
    "\n",
    "## What We're Doing\n",
    "\n",
    "Previously, we created a dataset of question/SQL query pairs and validated them against actual SQLite databases. Now we'll **fine-tune a language model** to accept a SQL schema and a natural language question, then output a correct SQLite query.\n",
    "\n",
    "## Why This Approach Works\n",
    "\n",
    "Fine-tuning allows us to:\n",
    "- Teach the model our specific database schemas and patterns\n",
    "- Improve accuracy on domain-specific SQL queries\n",
    "- Reduce hallucinations by training on validated examples\n",
    "\n",
    "## Dataset Format Strategy\n",
    "\n",
    "The first step is preparing our dataset in the **correct format**. We'll use ChatML format because:\n",
    "\n",
    "1. **Consistency**: Use the same format your base model was trained on (most modern models use chat format)\n",
    "2. **Portability**: This format works across multiple platforms (Axolotl, OpenAI fine-tuning, etc.)\n",
    "3. **Structure**: Clear separation of system instructions, user input, and expected output\n",
    "\n",
    "Our dataset structure:\n",
    "```json\n",
    "{\n",
    "    \"conversations\": [\n",
    "        {\"role\": \"system\", \"content\": \"Expert SQL developer instructions...\"},\n",
    "        {\"role\": \"user\", \"content\": \"Schema: [tables] Question: [user query]\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"SELECT * FROM table WHERE...\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This 3-message pattern teaches the model:\n",
    "- **System**: How to behave (SQL expert with SQLite constraints)\n",
    "- **User**: What information it receives (schema + question)  \n",
    "- **Assistant**: What we want it to output (correct SQL query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toX4WhNCvIW4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iK_4qRtOvIW6"
   },
   "outputs": [],
   "source": [
    "# If running this on colab, make sure to upload the parquet file and update the path.\n",
    "df = pd.read_parquet(\"../output/validated_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YifRBpdivIW6"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq6CVs-230ot"
   },
   "source": [
    "## Step 1: Schema Extraction\n",
    "\n",
    "To train our model effectively, we need to provide it with database schema information in a readable format. \n",
    "\n",
    "**Why schemas matter**: The model needs to understand:\n",
    "- What tables exist in the database\n",
    "- What columns are in each table  \n",
    "- Data types and constraints (PRIMARY KEY, NOT NULL, etc.)\n",
    "- Relationships between tables\n",
    "\n",
    "Let's create utilities to extract and format this schema information from our SQLite databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6OvetrTvIW7"
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "duckdb.execute(\"\"\"\n",
    "INSTALL sqlite;\n",
    "LOAD sqlite;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def query_sqlite(query: str, db_path: str) -> pd.DataFrame:\n",
    "    conn = duckdb.connect(db_path)\n",
    "    return conn.execute(query).fetch_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzeCD_vjvIW7"
   },
   "outputs": [],
   "source": [
    "path = df.db_path.iloc[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlahhsYdvIW7"
   },
   "outputs": [],
   "source": [
    "tables = query_sqlite(\"SHOW TABLES\", path)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdqZlSqwvIW7"
   },
   "outputs": [],
   "source": [
    "for table in tables.name:\n",
    "    print(table)\n",
    "    display(\n",
    "        t := query_sqlite(f\"DESCRIBE TABLE {table}\", path).dropna(how=\"all\", axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT4zfNoZvIW8"
   },
   "outputs": [],
   "source": [
    "def table_metadata(db_path: str) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get metadata for all tables in the database.\n",
    "    \"\"\"\n",
    "    tables = query_sqlite(\"SHOW TABLES\", db_path)\n",
    "    metadata = {}\n",
    "    for table in tables.name:\n",
    "        metadata[table] = query_sqlite(f\"DESCRIBE TABLE {table}\", db_path).dropna(\n",
    "            how=\"all\", axis=1\n",
    "        )\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weAvzz0DvIW8"
   },
   "outputs": [],
   "source": [
    "def format_schema_for_chat(metadata: dict[str, pd.DataFrame]) -> str:\n",
    "    \"\"\"\n",
    "    Format table metadata into a readable string for chat training.\n",
    "    \"\"\"\n",
    "    schema_lines = []\n",
    "\n",
    "    for table_name, df in metadata.items():\n",
    "        schema_lines.append(f\"Table: {table_name}\")\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            col_info = f\"  - {row['column_name']} ({row['column_type']}\"\n",
    "            if row[\"key\"] == \"PRI\":\n",
    "                col_info += \", PRIMARY KEY\"\n",
    "            elif pd.notna(row[\"key\"]) and row[\"key\"] != \"None\":\n",
    "                col_info += f\", {row['key']}\"\n",
    "            if row[\"null\"] == \"NO\":\n",
    "                col_info += \", NOT NULL\"\n",
    "            col_info += \")\"\n",
    "            schema_lines.append(col_info)\n",
    "\n",
    "        schema_lines.append(\"\")  # Add blank line between tables\n",
    "\n",
    "    return \"\\n\".join(schema_lines).strip()\n",
    "\n",
    "\n",
    "def create_chatml_dataset(df: pd.DataFrame) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Create ChatML conversations format dataset directly from DataFrame.\n",
    "    Includes system prompt with SQLite constraints.\n",
    "    \"\"\"\n",
    "    # SQLite-focused system prompt based on constraints from prompts.py\n",
    "    system_prompt = \"\"\"You are an expert SQL developer specializing in SQLite. Generate accurate SQL queries that follow these requirements:\n",
    "\n",
    "SQLITE REQUIREMENTS:\n",
    "1. Use only standard SQLite-compatible SQL syntax\n",
    "2. Avoid PostgreSQL-specific functions like INTERVAL - use date arithmetic instead\n",
    "3. Use SQLite date functions: date(), datetime(), julianday() for date calculations\n",
    "4. For \"recent month\" use: WHERE date_column >= date((SELECT MAX(date_column) FROM table_name), '-1 month')\n",
    "5. For \"past 30 days\" use: WHERE date_column >= date((SELECT MAX(date_column) FROM table_name), '-30 days')\n",
    "6. For rolling averages, use window functions or subqueries\n",
    "7. Use MAX(appropriate_date_column) to find the most recent date\n",
    "8. Use proper SQLite column types: INTEGER, TEXT, REAL, BLOB\n",
    "9. Handle foreign key relationships correctly with proper JOIN syntax\n",
    "\n",
    "QUERY STANDARDS:\n",
    "- Write clear, efficient queries\n",
    "- Use appropriate aggregation functions (COUNT, SUM, AVG, MAX, MIN)\n",
    "- Include proper GROUP BY and ORDER BY clauses when needed\n",
    "- Use table aliases for readability in complex queries\"\"\"\n",
    "\n",
    "    chatml_data = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating ChatML dataset\"):\n",
    "        # Skip invalid entries\n",
    "        if not row[\"is_valid\"]:\n",
    "            continue\n",
    "\n",
    "        # Get table metadata for this database\n",
    "        metadata = table_metadata(row[\"db_path\"])\n",
    "        schema_text = format_schema_for_chat(metadata)\n",
    "\n",
    "        # Create user message with schema and question\n",
    "        user_content = f\"Database Schema:\\n{schema_text}\\n\\nQuestion: {row['question']}\"\n",
    "\n",
    "        # Create ChatML conversation with system prompt\n",
    "        chatml_entry = {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": row[\"query\"]},\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        chatml_data.append(chatml_entry)\n",
    "\n",
    "    return chatml_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDoJswpY4V-P"
   },
   "source": [
    "## Step 2: Creating the ChatML Dataset\n",
    "\n",
    "Now we'll convert our validated question/query pairs into the ChatML format for fine-tuning.\n",
    "\n",
    "**Key components of our dataset creation**:\n",
    "\n",
    "1. **System Prompt**: Detailed instructions about SQLite syntax and constraints\n",
    "2. **User Message**: Database schema + natural language question\n",
    "3. **Assistant Response**: The validated SQL query\n",
    "\n",
    "**Important notes**:\n",
    "- We only include validated examples (`is_valid=True`)\n",
    "- We stratify the train/test split by database to ensure both sets have representative samples\n",
    "- Each conversation teaches the model one complete SQL generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9c898g2A_hc"
   },
   "outputs": [],
   "source": [
    "df_filtered = df[df.is_valid]\n",
    "train_df, test_df = train_test_split(\n",
    "    df_filtered, test_size=0.1, random_state=42, stratify=df_filtered.db_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zkKrLVDvIW8"
   },
   "outputs": [],
   "source": [
    "# Create the ChatML format dataset for Axolotl\n",
    "print(\"Creating ChatML format dataset for Axolotl...\")\n",
    "train_chatml_dataset = create_chatml_dataset(train_df)\n",
    "eval_chatml_dataset = create_chatml_dataset(test_df)\n",
    "\n",
    "print(f\"Created {len(train_chatml_dataset)} ChatML training examples\")\n",
    "print(f\"Created {len(eval_chatml_dataset)} ChatML evaluation examples\")\n",
    "\n",
    "# Save to JSONL file for Axolotl\n",
    "train_chatml_output_file = \"chatml_training_data.jsonl\"\n",
    "eval_chatml_output_file = \"chatml_evaluation_data.jsonl\"\n",
    "\n",
    "with open(train_chatml_output_file, \"w\") as f:\n",
    "    for entry in train_chatml_dataset:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Saved ChatML training data to {train_chatml_output_file}\")\n",
    "\n",
    "with open(eval_chatml_output_file, \"w\") as f:\n",
    "    for entry in eval_chatml_dataset:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"Saved ChatML evaluation data to {eval_chatml_output_file}\")\n",
    "\n",
    "# Display a sample ChatML entry\n",
    "print(\"\\nSample ChatML training entry:\")\n",
    "print(json.dumps(train_chatml_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fine-tuning with LoRA\n",
    "\n",
    "## What is LoRA?\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** is an efficient fine-tuning technique that:\n",
    "- Only trains a small number of additional parameters (~1% of the original model)\n",
    "- Maintains the original model weights unchanged\n",
    "- Reduces memory requirements and training time\n",
    "- Produces a small \"adapter\" that can be easily shared and applied\n",
    "\n",
    "## Why Use Google Colab?\n",
    "\n",
    "Fine-tuning requires significant computational resources:\n",
    "- **GPU Memory**: Need at least 8GB+ GPU memory (L4, T4, V100, A100)\n",
    "- **Flash Attention**: Speeds up training and reduces memory usage (L4 and A100 only)\n",
    "- **Cost Effective**: Colab Pro gives access to powerful GPUs without infrastructure setup\n",
    "\n",
    "## Training Strategy\n",
    "\n",
    "We'll use **Axolotl**, a popular fine-tuning framework that:\n",
    "- Handles data preprocessing automatically\n",
    "- Supports various model architectures and adapters\n",
    "- Provides sensible defaults for LoRA training\n",
    "- Integrates with HuggingFace for easy model sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "We need to install Axolotl with Flash Attention support. This installation:\n",
    "- **Requires GPU**: Flash Attention only works on CUDA GPUs\n",
    "- **Takes time**: Compiling Flash Attention can take 5-10 minutes\n",
    "- **Memory intensive**: Make sure you have sufficient GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qBO2ozuHvO0i"
   },
   "outputs": [],
   "source": [
    "# Note: must be connected to at least an L4 gpu (colab or other) to use flash-attn\n",
    "!uv pip install -Uqqq packaging setuptools wheel ninja\n",
    "!uv pip install -qqq --no-build-isolation \"axolotl[flash-attn,deepspeed]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Environment\n",
    "\n",
    "Configure HuggingFace token for model access and downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "30RQ8n4Tzf11"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Training Data\n",
    "\n",
    "Download our prepared ChatML datasets from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AieJHTf-BvWB",
    "outputId": "9f4de399-3acc-462b-86ef-c1c2599105ef"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define file URLs and local filenames\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/mgfrantz/CalTech-CTME-AramCo-2025/refs/heads/main/notebooks/chatml_evaluation_data.jsonl\",\n",
    "    \"https://raw.githubusercontent.com/mgfrantz/CalTech-CTME-AramCo-2025/refs/heads/main/notebooks/chatml_training_data.jsonl\",\n",
    "]\n",
    "filenames = [\n",
    "    \"chatml_evaluation_data.jsonl\",\n",
    "    \"chatml_training_data.jsonl\",\n",
    "]\n",
    "\n",
    "# Download files if they don't exist\n",
    "for url, filename in zip(urls, filenames):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        !wget {url} -O {filename}\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Configure Training\n",
    "\n",
    "This YAML configuration defines our training setup:\n",
    "\n",
    "**Model Configuration**:\n",
    "- `base_model`: Starting with Llama-3.2-1B (fast to train, good performance)\n",
    "- `load_in_8bit`: Reduces memory usage for training\n",
    "\n",
    "**LoRA Configuration**:\n",
    "- `lora_r`: 32 (rank of adaptation - higher = more parameters but better learning)\n",
    "- `lora_alpha`: 16 (scaling factor)\n",
    "- `lora_target_modules`: Which layers to adapt (all attention and MLP layers)\n",
    "\n",
    "**Training Configuration**:\n",
    "- `num_epochs`: 10 (how many times to see the full dataset)\n",
    "- `learning_rate`: 0.0002 (conservative rate to avoid overfitting)\n",
    "- `micro_batch_size`: 2 (small batches to fit in GPU memory)\n",
    "- `gradient_accumulation_steps`: 4 (effective batch size = 2 × 4 = 8)\n",
    "\n",
    "**Monitoring**:\n",
    "- `evals_per_epoch`: 4 (check validation loss 4 times per epoch)\n",
    "- `saves_per_epoch`: 1 (save checkpoints once per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8kFUIFR6Hbi",
    "outputId": "4e1c1623-2945-4dc1-ab71-4bbaba3660cf"
   },
   "outputs": [],
   "source": [
    "%%writefile axolotl.yaml\n",
    "base_model: NousResearch/Llama-3.2-1B\n",
    "\n",
    "load_in_8bit: true\n",
    "load_in_4bit: false\n",
    "strict: false\n",
    "adapter: lora\n",
    "# Added for bitsandbytes configuration\n",
    "# bnb_4bit_use_double_quant: true\n",
    "\n",
    "\n",
    "# Data config\n",
    "chat_template: llama3\n",
    "datasets:\n",
    "  - path: chatml_training_data.jsonl\n",
    "    type: chat_template\n",
    "    field_messages: conversations\n",
    "dataset_prepared_path: last_run_prepared\n",
    "\n",
    "test_datasets:\n",
    "  - path: chatml_evaluation_data.jsonl\n",
    "    type: chat_template\n",
    "    field_messages: conversations\n",
    "    split: train\n",
    "\n",
    "output_dir: ./outputs/lora-out\n",
    "\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "eval_sample_packing: false # with a larger eval dataset, we would do this, but we don't have a large enough one today.\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "lora_target_modules:\n",
    "  - gate_proj\n",
    "  - down_proj\n",
    "  - up_proj\n",
    "  - q_proj\n",
    "  - v_proj\n",
    "  - k_proj\n",
    "  - o_proj\n",
    "\n",
    "wandb_project:\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "num_epochs: 10\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16:\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "loss_watchdog_threshold: 5.0\n",
    "loss_watchdog_patience: 3\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 4\n",
    "eval_table_size:\n",
    "eval_max_new_tokens: 128\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "  pad_token: <|finetune_right_pad_id|>\n",
    "  eos_token: <|eot_id|>\n",
    "\n",
    "# Added for saving best checkpoint and pushing to Hugging Face Hub\n",
    "save_only_k_checkpoints: 1\n",
    "save_total_limit: 1\n",
    "load_best_model_at_end: true\n",
    "metric_for_best_model: eval_loss # Or any other metric you want to track\n",
    "greater_is_better: false # True if the metric should be maximized, False if minimized\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "# push_to_hub: false\n",
    "# hub_model_id: your_huggingface_username/your_model_name # Replace with your desired repo ID\n",
    "# hub_private_repo: false # Set to true if you want a private repo\n",
    "# hub_always_push: false\n",
    "# hub_strategy: every_save # \"end\" to push only at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess Data\n",
    "\n",
    "Axolotl preprocesses our JSONL files into its internal format for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fLmRDqOCTIS",
    "outputId": "fa74fa29-cf12-45ff-c69a-b06a430a89da"
   },
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "!axolotl preprocess axolotl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "This will begin the fine-tuning process. Expected behavior:\n",
    "\n",
    "**Training Progress**:\n",
    "- 10 epochs of training (may take 30-60 minutes on L4 GPU)\n",
    "- Loss should decrease over time\n",
    "- Validation evaluations every ~25% of each epoch\n",
    "\n",
    "**What to Watch For**:\n",
    "- **Training loss decreasing**: Model is learning\n",
    "- **Validation loss stable/decreasing**: Not overfitting\n",
    "- **GPU memory usage**: Should be stable throughout training\n",
    "\n",
    "**Troubleshooting**:\n",
    "- If OOM (Out of Memory): Reduce `micro_batch_size` to 1\n",
    "- If loss not decreasing: Check data format and increase learning rate\n",
    "- If overfitting: Reduce `num_epochs` or increase regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWb-L0YfvIW9",
    "outputId": "1fcdd546-402e-4ff7-e305-26922857686f"
   },
   "outputs": [],
   "source": [
    "!axolotl train axolotl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Optional - Merge LoRA Weights\n",
    "\n",
    "**What this does**: Combines the base model with LoRA adapter into a single model file\n",
    "\n",
    "**When to use**:\n",
    "- ✅ **For inference/deployment**: Single file is easier to use\n",
    "- ✅ **For sharing**: Recipients don't need both base model + adapter\n",
    "- ❌ **For further training**: Keep adapter separate for additional fine-tuning\n",
    "- ❌ **With multiple adapters**: If you have multiple adapters you want to serve from the same base model, it's better to keep them separate.\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Pros**: Simpler deployment, no adapter loading required\n",
    "- **Cons**: Larger file size, can't easily swap adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m axolotl.cli.merge_lora axolotl.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Upload to HuggingFace Hub\n",
    "\n",
    "**Sharing your model**:\n",
    "- Makes your fine-tuned model publicly available\n",
    "- Others can use it with `transformers` library or other inference libraries like vLLM.\n",
    "- Enables easy deployment and inference\n",
    "\n",
    "**What gets uploaded**:\n",
    "- LoRA adapter weights (`adapter_model.bin`)\n",
    "- Configuration files (`adapter_config.json`)\n",
    "- Training metadata and logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sfmtXqLzvc_",
    "outputId": "a1b46f03-8e36-4e3b-e853-0c5bdb015697"
   },
   "outputs": [],
   "source": [
    "# [optional] upload to hf hub\n",
    "import os\n",
    "\n",
    "username = \"mgfrantz\"\n",
    "repo_name = \"NousResearch-Llama-3.2-1B-ctme-sql-demo\"\n",
    "if not username or not repo_name:\n",
    "    username = input(\"Username: \")\n",
    "    repo_name = input(\"Repo name: \")\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "!rm -r outputs/lora-out/checkpoint-*\n",
    "!huggingface-cli upload {username}/{repo_name} ./outputs/lora-out/ ."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
