{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgfrantz/CalTech-CTME-AramCo-2025/blob/main/notebooks/02_fine_tune_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA9sUDBfwiM9"
      },
      "source": [
        "# Part 1: Dataset Preparation\n",
        "\n",
        "Previously, we created a dataset of question/sql query pairs and validated them against actual SQLite databases.\n",
        "Now that we have this data, our goal is to fine-tune a model to accept a sql schema and a quesiton and output a sqlite query.\n",
        "\n",
        "The first step in this process is preparing our dataset.\n",
        "While there are many [dataset formats](https://docs.axolotl.ai/docs/dataset-formats/), it's important to use the **same dataset format your base model was trained on.**\n",
        "So for example if you have a chat model, you should make sure your dataset is formatted for chat.\n",
        "This ensures you're not learning a new dataset format along with your task.\n",
        "A good way to do that is to look at the axolotl [examples](https://github.com/axolotl-ai-cloud/axolotl/tree/main/examples) - there are samples for most any model family you'd want to fine tune with sensible defaults.\n",
        "\n",
        "In our case, we will output a `.jsonl` file where each line contains the folowing information:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"conversations\": [\n",
        "        {\n",
        "            \"role\": <system, user, assistant>,\n",
        "            \"content\": \"some content\"\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "We will follow this general format:\n",
        "\n",
        "- Message 1: our system prompt with general instructions\n",
        "- Message 2: our user message with database schema and user query\n",
        "- Message 3: our validated sql output (what we want to tune the model to output)\n",
        "\n",
        "This format is useful for almost any chat model, even proprietary for use in proprietarty fine-tuning ([openai docs](https://platform.openai.com/docs/guides/supervised-fine-tuning?formatting=jsonl)).\n",
        "This means you can prepare your data once, and fine-tune and compare many different models.\n",
        "\n",
        "Let's start by loading and observing our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toX4WhNCvIW4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iK_4qRtOvIW6"
      },
      "outputs": [],
      "source": [
        "# If running this on colab, make sure to upload the parquet file and update the path.\n",
        "df = pd.read_parquet(\"../output/validated_dataset.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YifRBpdivIW6",
        "outputId": "d4a3ea68-dae0-4c60-cad1-39f179fc8a86"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6CVs-230ot"
      },
      "source": [
        "Now, let's create some utilities to get the schema of the database as a string so we can pass it to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c6OvetrTvIW7"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "duckdb.execute(\"\"\"\n",
        "INSTALL sqlite;\n",
        "LOAD sqlite;\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "def query_sqlite(query: str, db_path: str) -> pd.DataFrame:\n",
        "    conn = duckdb.connect(db_path)\n",
        "    return conn.execute(query).fetch_df()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzeCD_vjvIW7",
        "outputId": "ab8cab69-bf94-4527-817d-5de3cddea4a8"
      },
      "outputs": [],
      "source": [
        "path = df.db_path.iloc[0]\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlahhsYdvIW7",
        "outputId": "2b4e1820-6e2a-4ff3-d54a-e76d4d12ebc7"
      },
      "outputs": [],
      "source": [
        "tables = query_sqlite(\"SHOW TABLES\", path)\n",
        "tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdqZlSqwvIW7",
        "outputId": "2f7216b3-1cdb-4213-8a7d-459b61841c21"
      },
      "outputs": [],
      "source": [
        "for table in tables.name:\n",
        "    print(table)\n",
        "    display(\n",
        "        t := query_sqlite(f\"DESCRIBE TABLE {table}\", path).dropna(how=\"all\", axis=1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DT4zfNoZvIW8"
      },
      "outputs": [],
      "source": [
        "def table_metadata(db_path: str) -> dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Get metadata for all tables in the database.\n",
        "    \"\"\"\n",
        "    tables = query_sqlite(\"SHOW TABLES\", db_path)\n",
        "    metadata = {}\n",
        "    for table in tables.name:\n",
        "        metadata[table] = query_sqlite(f\"DESCRIBE TABLE {table}\", db_path).dropna(\n",
        "            how=\"all\", axis=1\n",
        "        )\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "weAvzz0DvIW8"
      },
      "outputs": [],
      "source": [
        "def format_schema_for_chat(metadata: dict[str, pd.DataFrame]) -> str:\n",
        "    \"\"\"\n",
        "    Format table metadata into a readable string for chat training.\n",
        "    \"\"\"\n",
        "    schema_lines = []\n",
        "\n",
        "    for table_name, df in metadata.items():\n",
        "        schema_lines.append(f\"Table: {table_name}\")\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            col_info = f\"  - {row['column_name']} ({row['column_type']}\"\n",
        "            if row[\"key\"] == \"PRI\":\n",
        "                col_info += \", PRIMARY KEY\"\n",
        "            elif pd.notna(row[\"key\"]) and row[\"key\"] != \"None\":\n",
        "                col_info += f\", {row['key']}\"\n",
        "            if row[\"null\"] == \"NO\":\n",
        "                col_info += \", NOT NULL\"\n",
        "            col_info += \")\"\n",
        "            schema_lines.append(col_info)\n",
        "\n",
        "        schema_lines.append(\"\")  # Add blank line between tables\n",
        "\n",
        "    return \"\\n\".join(schema_lines).strip()\n",
        "\n",
        "\n",
        "def create_chatml_dataset(df: pd.DataFrame) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Create ChatML conversations format dataset directly from DataFrame.\n",
        "    Includes system prompt with SQLite constraints.\n",
        "    \"\"\"\n",
        "    # SQLite-focused system prompt based on constraints from prompts.py\n",
        "    system_prompt = \"\"\"You are an expert SQL developer specializing in SQLite. Generate accurate SQL queries that follow these requirements:\n",
        "\n",
        "SQLITE REQUIREMENTS:\n",
        "1. Use only standard SQLite-compatible SQL syntax\n",
        "2. Avoid PostgreSQL-specific functions like INTERVAL - use date arithmetic instead\n",
        "3. Use SQLite date functions: date(), datetime(), julianday() for date calculations\n",
        "4. For \"recent month\" use: WHERE date_column >= date((SELECT MAX(date_column) FROM table_name), '-1 month')\n",
        "5. For \"past 30 days\" use: WHERE date_column >= date((SELECT MAX(date_column) FROM table_name), '-30 days')\n",
        "6. For rolling averages, use window functions or subqueries\n",
        "7. Use MAX(appropriate_date_column) to find the most recent date\n",
        "8. Use proper SQLite column types: INTEGER, TEXT, REAL, BLOB\n",
        "9. Handle foreign key relationships correctly with proper JOIN syntax\n",
        "\n",
        "QUERY STANDARDS:\n",
        "- Write clear, efficient queries\n",
        "- Use appropriate aggregation functions (COUNT, SUM, AVG, MAX, MIN)\n",
        "- Include proper GROUP BY and ORDER BY clauses when needed\n",
        "- Use table aliases for readability in complex queries\"\"\"\n",
        "\n",
        "    chatml_data = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Creating ChatML dataset\"):\n",
        "        # Skip invalid entries\n",
        "        if not row[\"is_valid\"]:\n",
        "            continue\n",
        "\n",
        "        # Get table metadata for this database\n",
        "        metadata = table_metadata(row[\"db_path\"])\n",
        "        schema_text = format_schema_for_chat(metadata)\n",
        "\n",
        "        # Create user message with schema and question\n",
        "        user_content = f\"Database Schema:\\n{schema_text}\\n\\nQuestion: {row['question']}\"\n",
        "\n",
        "        # Create ChatML conversation with system prompt\n",
        "        chatml_entry = {\n",
        "            \"conversations\": [\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_content},\n",
        "                {\"role\": \"assistant\", \"content\": row[\"query\"]},\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        chatml_data.append(chatml_entry)\n",
        "\n",
        "    return chatml_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDoJswpY4V-P"
      },
      "source": [
        "Now that we've created our utilities, let's complete our chatml dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_filtered = df[df.is_valid]\n",
        "train_df, test_df = train_test_split(\n",
        "    df_filtered, test_size=0.1, random_state=42, stratify=df_filtered.db_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c3aee61fbfe3403fb269c7d049d7d09e"
          ]
        },
        "id": "_zkKrLVDvIW8",
        "outputId": "65a9295a-63a8-4a39-e649-364565610696"
      },
      "outputs": [],
      "source": [
        "# Create the ChatML format dataset for Axolotl\n",
        "print(\"Creating ChatML format dataset for Axolotl...\")\n",
        "train_chatml_dataset = create_chatml_dataset(train_df)\n",
        "eval_chatml_dataset = create_chatml_dataset(test_df)\n",
        "\n",
        "print(f\"Created {len(train_chatml_dataset)} ChatML training examples\")\n",
        "print(f\"Created {len(eval_chatml_dataset)} ChatML evaluation examples\")\n",
        "\n",
        "# Save to JSONL file for Axolotl\n",
        "train_chatml_output_file = \"chatml_training_data.jsonl\"\n",
        "eval_chatml_output_file = \"chatml_evaluation_data.jsonl\"\n",
        "\n",
        "with open(train_chatml_output_file, \"w\") as f:\n",
        "    for entry in train_chatml_dataset:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Saved ChatML training data to {train_chatml_output_file}\")\n",
        "\n",
        "with open(eval_chatml_output_file, \"w\") as f:\n",
        "    for entry in eval_chatml_dataset:\n",
        "        f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "\n",
        "print(f\"Saved ChatML evaluation data to {eval_chatml_output_file}\")\n",
        "\n",
        "# Display a sample ChatML entry\n",
        "print(\"\\nSample ChatML training entry:\")\n",
        "print(json.dumps(train_chatml_dataset[0], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qBO2ozuHvO0i"
      },
      "outputs": [],
      "source": [
        "# Note: must be connected to at least an L4 gpu (colab or other) to use flash-attn\n",
        "!uv pip install -Uqqq packaging setuptools wheel ninja\n",
        "!uv pip install -qqq --no-build-isolation \"axolotl[flash-attn,deepspeed]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "30RQ8n4Tzf11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8kFUIFR6Hbi",
        "outputId": "244c42fa-5aa6-46a7-9ba9-59e100ef63bf"
      },
      "outputs": [],
      "source": [
        "%%writefile axolotl.yaml\n",
        "base_model: NousResearch/Llama-3.2-1B\n",
        "\n",
        "load_in_8bit: true\n",
        "load_in_4bit: false\n",
        "strict: false\n",
        "adapter: lora\n",
        "# Added for bitsandbytes configuration\n",
        "# bnb_4bit_use_double_quant: true\n",
        "\n",
        "\n",
        "# Data config\n",
        "chat_template: llama3\n",
        "datasets:\n",
        "  - path: chatml_training_data.jsonl\n",
        "    type: chat_template\n",
        "    field_messages: conversations\n",
        "dataset_prepared_path: last_run_prepared\n",
        "\n",
        "test_datasets:\n",
        "  - path: chatml_evaluation_data.jsonl\n",
        "    type: chat_template\n",
        "    field_messages: conversations\n",
        "\n",
        "output_dir: ./outputs/lora-out\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "eval_sample_packing: false # with a larger eval dataset, we would do this, but we don't have a large enough one today.\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "lora_target_modules:\n",
        "  - gate_proj\n",
        "  - down_proj\n",
        "  - up_proj\n",
        "  - q_proj\n",
        "  - v_proj\n",
        "  - k_proj\n",
        "  - o_proj\n",
        "\n",
        "wandb_project:\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_name:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 4\n",
        "micro_batch_size: 2\n",
        "num_epochs: 1\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: auto\n",
        "fp16:\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "loss_watchdog_threshold: 5.0\n",
        "loss_watchdog_patience: 3\n",
        "\n",
        "warmup_steps: 10\n",
        "evals_per_epoch: 4\n",
        "eval_table_size:\n",
        "eval_max_new_tokens: 128\n",
        "saves_per_epoch: 1\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: <|finetune_right_pad_id|>\n",
        "  eos_token: <|eot_id|>\n",
        "\n",
        "# Added for saving best checkpoint and pushing to Hugging Face Hub\n",
        "save_only_k_checkpoints: 1\n",
        "save_total_limit: 1\n",
        "load_best_model_at_end: true\n",
        "metric_for_best_model: eval_loss # Or any other metric you want to track\n",
        "greater_is_better: false # True if the metric should be maximized, False if minimized\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "# push_to_hub: false\n",
        "# hub_model_id: your_huggingface_username/your_model_name # Replace with your desired repo ID\n",
        "# hub_private_repo: false # Set to true if you want a private repo\n",
        "# hub_always_push: false\n",
        "# hub_strategy: every_save # \"end\" to push only at the end of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWb-L0YfvIW9",
        "outputId": "5ed7033e-75c4-4e2f-9f25-9aa9ba4efe0f"
      },
      "outputs": [],
      "source": [
        "!axolotl train axolotl.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y3eFNBvnvIW9"
      },
      "outputs": [],
      "source": [
        "# [optional] merge weights\n",
        "# !python -m axolotl.cli.merge_lora axolotl.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sfmtXqLzvc_",
        "outputId": "72cd53c0-04af-40d9-f48c-1861aa7400af"
      },
      "outputs": [],
      "source": [
        "# [optional] upload to hf hub\n",
        "import os\n",
        "\n",
        "username = \"mgfrantz\"\n",
        "repo_name = \"cmte-demo\"\n",
        "if not username or not repo_name:\n",
        "    username = input(\"Username: \")\n",
        "    repo_name = input(\"Repo name: \")\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "!rm -r outputs/lora-out/checkpoint-*\n",
        "!huggingface-cli upload {username}/{repo_name} ./outputs/lora-out/ ."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
