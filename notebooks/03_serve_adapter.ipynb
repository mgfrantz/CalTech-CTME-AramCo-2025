{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mgfrantz/CalTech-CTME-AramCo-2025/blob/main/notebooks/03_serve_adapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving and Testing Your Fine-tuned SQL Model\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "After fine-tuning our LoRA adapter in the previous notebook, we now need to:\n",
    "\n",
    "1. **Serve the model** - Set up an inference server that can load both the base model and our LoRA adapter\n",
    "2. **Test the adapter** - Validate that our fine-tuning improved SQL generation quality\n",
    "3. **Compare performance** - See how the fine-tuned model performs vs. the base model\n",
    "\n",
    "## Why Use vLLM for Serving?\n",
    "\n",
    "**vLLM** is a high-performance inference server that offers:\n",
    "\n",
    "- **Fast inference**: Optimized for serving large language models\n",
    "- **LoRA support**: Can dynamically load different adapters without restarting\n",
    "- **OpenAI-compatible API**: Easy integration with existing tools\n",
    "- **Multi-adapter serving**: Serve multiple LoRA adapters simultaneously\n",
    "- **Efficient memory usage**: Shares base model weights across adapters\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n",
    "│   Base Model    │    │   LoRA Adapter   │    │   vLLM Server   │\n",
    "│ (Llama-3.2-1B)  │ +  │  (SQL Tuning)    │ =  │  (Inference)    │\n",
    "│                 │    │                  │    │                 │\n",
    "└─────────────────┘    └──────────────────┘    └─────────────────┘\n",
    "                                                        │\n",
    "                                                        ▼\n",
    "                                               ┌─────────────────┐\n",
    "                                               │  OpenAI API     │\n",
    "                                               │  Compatible     │\n",
    "                                               │  Endpoint       │\n",
    "                                               └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "We need several key packages for serving and testing:\n",
    "\n",
    "- **vLLM**: High-performance inference server with LoRA support\n",
    "- **litellm**: Unified API client for different LLM providers \n",
    "- **bitsandbytes**: Efficient quantization for memory optimization\n",
    "\n",
    "**Installation time**: ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install -qqq vllm litellm bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Start the vLLM Server\n",
    "\n",
    "**⚠️ IMPORTANT**: You need to run this command in a **separate terminal** because it starts a server process.\n",
    "\n",
    "### Command Breakdown:\n",
    "\n",
    "```bash\n",
    "vllm serve NousResearch/Llama-3.2-1B \\\n",
    "    --max-model-len 2048 \\\n",
    "    --enable-lora \\\n",
    "    --max-lora-rank 32 \\\n",
    "    --lora-modules sql-lora=mgfrantz/NousResearch-Llama-3.2-1B-ctme-sql-demo\n",
    "```\n",
    "\n",
    "**Parameters explained**:\n",
    "\n",
    "- `NousResearch/Llama-3.2-1B`: Base model to load\n",
    "- `--max-model-len 2048`: Maximum sequence length (matches our training config)\n",
    "- `--enable-lora`: Enable LoRA adapter support\n",
    "- `--max-lora-rank 32`: Maximum LoRA rank (matches our training rank)\n",
    "- `--lora-modules sql-lora=<adapter_path>`: Maps adapter name to HuggingFace model ID\n",
    "\n",
    "### What Happens When You Run This:\n",
    "\n",
    "1. **Downloads base model** (~2.5GB) - first time only\n",
    "2. **Downloads LoRA adapter** (~60MB) - much smaller!\n",
    "3. **Loads model into GPU memory** - requires ~4GB GPU RAM\n",
    "4. **Starts HTTP server** on `http://localhost:8000`\n",
    "5. **Ready for inference** - OpenAI-compatible API endpoints\n",
    "\n",
    "### Expected Output:\n",
    "```\n",
    "INFO:     Started server process\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000\n",
    "```\n",
    "\n",
    "### Manual Steps:\n",
    "1. Open a new terminal in Colab (click the terminal icon)\n",
    "2. Run the vllm serve command above\n",
    "3. Wait for \"Application startup complete\" message\n",
    "4. Leave terminal running and return to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download and Load Test Data\n",
    "\n",
    "We'll use the same evaluation dataset we created during training to test our model's performance.\n",
    "It's the same json in the chatml format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from rich import print\n",
    "\n",
    "# Define file URLs and local filenames\n",
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/mgfrantz/CalTech-CTME-AramCo-2025/refs/heads/main/notebooks/chatml_evaluation_data.jsonl\",\n",
    "    \"https://raw.githubusercontent.com/mgfrantz/CalTech-CTME-AramCo-2025/refs/heads/main/notebooks/chatml_training_data.jsonl\",\n",
    "]\n",
    "filenames = [\n",
    "    \"chatml_evaluation_data.jsonl\",\n",
    "    \"chatml_training_data.jsonl\",\n",
    "]\n",
    "\n",
    "# Download files if they don't exist\n",
    "for url, filename in zip(urls, filenames):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        !wget {url} -O {filename}\n",
    "    else:\n",
    "        print(f\"{filename} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_conversations = []\n",
    "with open('chatml_evaluation_data.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        eval_conversations.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare a Test Case and Test the Fine-tuned Model\n",
    "\n",
    "Extract the input messages (system + user) and expected answer from our test example.\n",
    "\n",
    "**Test structure**:\n",
    "- **Messages**: System prompt + user question with schema\n",
    "- **Expected answer**: The correct SQL query our model should generate\n",
    "\n",
    "Set up connection parameters for our vLLM server:\n",
    "\n",
    "- **vllm_url**: Local server endpoint (default vLLM port is 8000)\n",
    "- **model**: Specify our LoRA adapter name (`sql-lora` as defined in the serve command). We use `openai` to tell `litellm` that this is an openai-compatible server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = eval_conversations[0].get('conversations')\n",
    "messages = convo[:-1]\n",
    "answer = convo[-1]\n",
    "\n",
    "print(\"Messages:\")\n",
    "print(messages)\n",
    "\n",
    "print(\"\\n\\nExpected answer:\")\n",
    "print(answer.get('content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_url = 'http://localhost:8000/v1/'\n",
    "model = 'openai/sql-lora'\n",
    "\n",
    "resp = completion(\n",
    "    model,\n",
    "    messages,\n",
    "    base_url=vllm_url,\n",
    "    api_key='not-needed',\n",
    "    max_completion_tokens=128\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vCptqWA455l",
    "outputId": "ded3dcb0-feb9-49ee-f246-f91e973da19c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RGeOsNx7ceO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy1C7Qysl7DjYD7xLMCReV",
   "gpuType": "L4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
